{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step by step to do Super Resolution Convolutional Neural Network\n",
    "## Implement an image super-resolution technique in TensorFlow\n",
    "\n",
    "> I'm still not done on doing explanatory journey with this code. Some still taken from the original website. \n",
    "\n",
    "This step-by-step approach will be run through the methodology based on this link below\n",
    "\n",
    "https://dzlab.github.io/notebooks/tensorflow/generative/artistic/2021/05/10/Super_Resolution_SRCNN.html\n",
    "\n",
    "The run through methodology I used made by : [dzlab](https://github.com/dzlab/notebooks/blob/master/_notebooks/2021-05-10-Super_Resolution_SRCNN.ipynb) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "thesis_super_resolution_p3_10\n"
     ]
    }
   ],
   "source": [
    "# Just make sure this REPL run thesis_super_resolution\n",
    "\n",
    "import os\n",
    "print(os.environ['CONDA_DEFAULT_ENV'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing Packages\n",
    "import os\n",
    "import pathlib\n",
    "from glob import glob\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.preprocessing.image import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random seed for reproducibility???\n",
    "SEED = 31\n",
    "np.random.seed(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "\n",
    "# Dataset-related constant\n",
    "\n",
    "file_path = (pathlib.Path('/content') / 'images' / '*.png')\n",
    "file_pattern = str(file_path)\n",
    "dataset_path = [*glob(file_pattern)]\n",
    "\n",
    "SUBSET_SIZE = 1000\n",
    "dataset_path = np.random.choice(dataset_path, SUBSET_SIZE)\n",
    "\n",
    "# Parameter-based constant\n",
    "SCALE = 2.0\n",
    "INPUT_DIM = 33\n",
    "LABEL_SIZE = 21\n",
    "PAD = int((INPUT_DIM - LABEL_SIZE) / 2.0)\n",
    "STRIDE = 14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example Immage from the dataset\n",
    "\n",
    "path = np.random.choice(dataset_path)\n",
    "img = plt.imread(path)\n",
    "plt.imshow(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Support Functions \n",
    "In the references, I saw some support functions to help with data pre-processing:\n",
    "\n",
    "### 1. Resize image\n",
    "As its name, this function will resize an image.\n",
    "\n",
    "This function requires these parameters below:\n",
    "\n",
    "| Parameter | Description |\n",
    "|---|---|\n",
    "| `image_array` | This parameter specified for a converted image array. So basically, the image that already transformed into multi-dimensional array. |\n",
    "| `factor` | The percentage of resized image. Giving value of below than 1 means that the resized image will be bigger than the original image. |\n",
    "\n",
    "This function return a resized image array data.\n",
    "\n",
    "### 2. Downsize Upsize Image\n",
    "This function will generate low resolution image by downsize and upsizing it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# resize_image\n",
    "def resize_image(image_array, factor):\n",
    "    original_image = Image.fromarray(image_array)\n",
    "\n",
    "    new_size = np.array(original_image.size) * factor\n",
    "    new_size = new_size.astype(np.int32)\n",
    "    new_size = tuple(new_size)\n",
    "\n",
    "    resized = original_image.resize(new_size)\n",
    "    resized = img_to_array(resized)\n",
    "    resized = resized.astype(np.uint8)\n",
    "    \n",
    "    return resized\n",
    "\n",
    "def downsize_upsize_image(image, scale):\n",
    "    scaled = resize_image(image, 1.0 / scale)\n",
    "    scaled = resize_image(scaled, scale) # In the reference, the scale is divided by 1.0. What changes over it?\n",
    "\n",
    "    return scaled\n",
    "\n",
    "def tight_crop_image(image, scale):\n",
    "    height, width = image.shape[:2]\n",
    "\n",
    "    width -= int(width % scale)\n",
    "    height -= int(height % scale)\n",
    "\n",
    "    return image[:height, :width]\n",
    "\n",
    "def crop_input(image, x, y)\n",
    "    x_slice = slice(x, x + INPUT_DIM)\n",
    "    y_slice = slice(y, y + INPUT_DIM)\n",
    "    return image[y_slice, x_slice]\n",
    "\n",
    "def crop_output(image, x, y)\n",
    "    x_slice = slice(x + PAD, x + PAD + LABEL_SIZE)\n",
    "    y_slice = slice(y + PAD, y + PAD + LABEL_SIZE)\n",
    "    \n",
    "    return image[y_slice, x_slice]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting data from the disk\n",
    "\n",
    "-> I'm still not sure what to do with this step, but it says:\n",
    "\n",
    "> Now, lets build the dataset by reading the input images, generating a low resolution version, sliding a window on this low resolution image as well as the original image to generate patches for training. We will save the patches to disk and later build a training data generator that will load them from > disk in batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for image_path in tqdm(dataset_path):\n",
    "    filename = pathlib.Path(image_path).stem\n",
    "    image = load_img(image_path)\n",
    "    image = img_to_array(image)\n",
    "    image = image.astype(np.uint8)\n",
    "    image = tight_crop_image(image, SCALE)\n",
    "    scaled = downsize_upsize_image(image, SCALE)\n",
    "\n",
    "    height, width = image.shape[:2]\n",
    "\n",
    "    for y in range(0, height - INPUT_DIM + 1, STRIDE):\n",
    "        for x in range(0, width - INPUT_DIM + 1, STRIDE):\n",
    "            crop = crop_input(scaled, x, y)\n",
    "            target = crop_output(image, x, y)\n",
    "            np.save(f'data/{filename}_{x}_{y}_input.np', crop)\n",
    "            np.save(f'data/{filename}_{x}_{y}_output.np', target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "> We cannot hold all the patches in memory hence we saved to disk in the previous step. Now we need a dataset loader that will load a patch and its label and feed them to the network during traning in batches. This is achieved with the PatchesDataset class (check this example to learn more about generators - [link](https://dzlab.github.io/dltips/en/keras/data-generator/))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatchesDataset(tf.keras.utils.Sequence):\n",
    "    def __init__(self, batch_size, *args, **kwargs):\n",
    "        self.batch_size = batch_size\n",
    "        self.input = [*glob('data/*_input.np.npy')]\n",
    "        self.output = [*glob('data/*_output.np.npy')]\n",
    "        self.input.sort()\n",
    "        self.output.sort()\n",
    "        self.total_data = len(self.input)\n",
    "\n",
    "    def __len__(self):\n",
    "        # returns the number of batches\n",
    "        return int(self.total_data / self.batch_size)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # returns one batch\n",
    "        indices = self.random_indices()\n",
    "        input = np.array([np.load(self.input[idx]) for idx in indices])\n",
    "        output = np.array([np.load(self.output[idx]) for idx in indices])\n",
    "        return input, output\n",
    "\n",
    "    def random_indices(self):\n",
    "        return np.random.choice(list(range(self.total_data)), self.batch_size, p=np.ones(self.total_data)/self.total_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Define a batch size based on how much memory available on your GPU and create an instance of the dataset generator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 1024\n",
    "train_ds = PatchesDataset(BATCH_SIZE)\n",
    "len(train_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> You can see the shape of the training batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input, output = train_ds[0]\n",
    "input.shape, output.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model \n",
    "\n",
    "> The architecture of the SRCNN model is very simple, it has only convolutional layers, one to downsize the input and extract image features and a later one to upside to generate the output image. The following helper function is used to create an instance of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(height, width, depth):\n",
    "    input = Input(shape=(height, width, depth))\n",
    "    x = Conv2D(filters=64, kernel_size=(9, 9), kernel_initializer='he_normal')(input)\n",
    "    x = ReLU()(x)\n",
    "    x = Conv2D(filters=32, kernel_size=(1, 1), kernel_initializer='he_normal')(x)\n",
    "    x = ReLU()(x)\n",
    "    output = Conv2D(filters=depth, kernel_size=(5, 5), kernel_initializer='he_normal')(x)\n",
    "    return Model(input, output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> To train the network we will use Adam as optimizer with learning rate decay. Also, as the problem we try to train the network for is a regression problem (we want predict the high resolution pixels) we pick MSE as a loss function, this will make the model learn the filters that correctly map patches from low to high resolution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 12\n",
    "optimizer = Adam(learning_rate=1e-3, decay=1e-3 / EPOCHS)\n",
    "model = create_model(INPUT_DIM, INPUT_DIM, 3)\n",
    "model.compile(loss='mse', optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> You can see how the model is small but astonishly it will be able to achieve great results once trained for enough time, we will train it for 12 epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.utils.plot_model(model, show_shapes = True, rankdir='LR')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Create a callback that saves the model's weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_path = \"training/cp.ckpt\"\n",
    "checkpoint_dir = os.path.dirname(checkpoint_path)\n",
    "cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path, save_weights_only=True, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now finally, we can train the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(train_ds, epochs=EPOCHS, callbacks=[cp_callback])"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "9ad69851df0cb4453262857d3b1a61ab8eb1167f128089e34c7816a6b823fb9b"
  },
  "kernelspec": {
   "display_name": "Python 3.11.5 64-bit (windows store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
