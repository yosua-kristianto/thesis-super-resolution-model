{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Data Pre-Processing\n",
    "\n",
    "This notebook contains my version of image data pre-processing. The dataset I used to train the model is [FUNSD Dataset](https://guillaumejaume.github.io/FUNSD/dataset.zip). I downloaded the dataset, and save it to my local directory. \n",
    "\n",
    "The main goal of this notebook is to create a directory containing the bad version of my training dataset, and actually labeling it with the good version image (original image)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing things\n",
    "import os\n",
    "import numpy\n",
    "from numpy import asarray\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "from datetime import datetime\n",
    "from tensorflow.keras.utils import img_to_array\n",
    "from tensorflow.keras.preprocessing.image import load_img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we define some parameters:\n",
    "\n",
    "| Parameter Name | Description |\n",
    "|---|---|\n",
    "| `SCALE` | Resizing scale factor |\n",
    "| `INPUT_DIM` | Input and Output patch sizes |\n",
    "| `PAD` | Padding that need to be added to output patches |\n",
    "| `STRIDE` | Larger STRIDE will result in higher pixel skipping. Which will reduce more image quality. On the dzlab's github, `STRIDE` explained as *\"the stride which is the number of pixels we'll slide both in the horizontal and vertical axes to extract patches\"* |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "SCALE = 2.0\n",
    "INPUT_DIM = 33\n",
    "LABEL_SIZE = 21\n",
    "PAD = int((INPUT_DIM - LABEL_SIZE) / 2.0)\n",
    "STRIDE = 14"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Making support functions\n",
    "This part is making support function to actually reducing the image quality for the testing data. \n",
    "\n",
    "List functions to be made:\n",
    "\n",
    "1. `resize_image` \n",
    "As its name suggests, this function will resize the image by the specified factor. \n",
    "\n",
    "If you want to downsample the image, you can set the factor by 1 / x or x / 100.\n",
    "If you want to upsample the image, you can set the factor by x\n",
    "\n",
    "2. convert_image_to_array\n",
    "As its name suggests, this function will convert a raw image data to a numpy array. You may notice that this function is only contains 1 line, but trust me, as a skilled-issue user (me), you may want to do this for better understanding of the function.\n",
    "\n",
    "3. downsize_upsize_image\n",
    "This function will downsample, and then upsample the image. It says, if this happen, for some reason the image will be start degraded on its quality.\n",
    "\n",
    "4. tight_crop_image\n",
    "This function will \n",
    "\n",
    "5. crop_input\n",
    "This function will slice through the input image to the destinated dimension.\n",
    "\n",
    "6. crop_output\n",
    "This function will slice through the target image to the destinated dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# resize_image\n",
    "def resize_image(image_array, factor):\n",
    "    original_image = Image.fromarray(image_array)\n",
    "\n",
    "    new_size = numpy.array(original_image.size) * factor\n",
    "    new_size = new_size.astype(numpy.int32)\n",
    "    new_size = tuple(new_size)\n",
    "\n",
    "    resized = original_image.resize(new_size)\n",
    "    resized = img_to_array(resized)\n",
    "    resized = resized.astype(numpy.uint8)\n",
    "    \n",
    "    return resized\n",
    "\n",
    "\n",
    "# convert image to array\n",
    "# This function will convert an image to numpy array spatial data.\n",
    "# @param\n",
    "#  - str image_path\n",
    "# \n",
    "# @return \n",
    "#  - numpy array\n",
    "def convert_image_to_array(image_path):\n",
    "    return asarray(Image.open(image_path))\n",
    "\n",
    "def downsize_upsize_image(image, scale):\n",
    "    scaled = resize_image(image, 1.0 / scale)\n",
    "    scaled = resize_image(scaled, scale) # In the reference, the scale is divided by 1.0. What changes over it?\n",
    "\n",
    "    return scaled\n",
    "\n",
    "def tight_crop_image(image, scale):\n",
    "    height, width = image.shape[:2]\n",
    "\n",
    "    width -= int(width % scale)\n",
    "    height -= int(height % scale)\n",
    "\n",
    "    return image[:height, :width]\n",
    "\n",
    "def crop_input(image, x, y):\n",
    "    x_slice = slice(x, x + INPUT_DIM)\n",
    "    y_slice = slice(y, y + INPUT_DIM)\n",
    "    return image[y_slice, x_slice]\n",
    "\n",
    "def crop_output(image, x, y):\n",
    "    x_slice = slice(x + PAD, x + PAD + LABEL_SIZE)\n",
    "    y_slice = slice(y + PAD, y + PAD + LABEL_SIZE)\n",
    "    \n",
    "    return image[y_slice, x_slice]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Making the bad and the good image version.\n",
    "Since the code from dzlab mainly uses Google Collab, and unfortunately I'm using Windows, I have to kind of change how the code is interacting with the image entirely.\n",
    "\n",
    "This algorithm below explains my methodology:\n",
    "\n",
    "1. Set the directory using Path from pathlib\n",
    "2. For every file within the directory:\n",
    "    > 1. Pre-Process the image using keras img_to_array\n",
    "    > 2. Save the original image array file to disk.\n",
    "    > 3. Making LowRes images from the normal quality image.\n",
    "    > 4. Saving the LowRes images to disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image():\n",
    "    directory = Path(\"D:\\\\storage\\\\training\\\\dataset\\\\testing_data\\\\images\")\n",
    "\n",
    "    for file in os.listdir(directory):\n",
    "        full_path = os.path.join(directory, file)\n",
    "\n",
    "        # Call out 2.1\n",
    "        image = img_to_array(load_img(full_path))\n",
    "        image = image.astype(numpy.uint8)\n",
    "\n",
    "        # Generate target filename\n",
    "        file_name = datetime.now().strftime(\"%Y%m%d%H%M%S\") + \"_\" + file + \".numpy\"\n",
    "        \n",
    "        # Call out 2.2\n",
    "        numpy.save(\"../resources/np_image_original/original_\" + file_name, image)\n",
    "\n",
    "        # Call out 2.3\n",
    "\n",
    "        image = tight_crop_image(image, SCALE)\n",
    "        scaled = downsize_upsize_image(image, SCALE)\n",
    "\n",
    "        height, width = image.shape[:2]\n",
    "\n",
    "        for y in range(0, height - INPUT_DIM + 1, STRIDE):\n",
    "            for x in range(0, width - INPUT_DIM + 1, STRIDE):\n",
    "                crop = crop_input(scaled, x, y)\n",
    "                target = crop_output(image, x, y)\n",
    "\n",
    "                numpy.save(\"../resources/np_image_input/input_\" + file_name + '.np', crop)\n",
    "                numpy.save(\"../resources/np_image_output/target_\" + file_name + '.np', target)\n",
    "\n",
    "\n",
    "        print(\"Processed \" + full_path + \" into spatial data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed D:\\storage\\training\\dataset\\testing_data\\images\\82092117.png into spatial data\n",
      "Processed D:\\storage\\training\\dataset\\testing_data\\images\\82200067_0069.png into spatial data\n",
      "Processed D:\\storage\\training\\dataset\\testing_data\\images\\82250337_0338.png into spatial data\n",
      "Processed D:\\storage\\training\\dataset\\testing_data\\images\\82251504.png into spatial data\n",
      "Processed D:\\storage\\training\\dataset\\testing_data\\images\\82252956_2958.png into spatial data\n",
      "Processed D:\\storage\\training\\dataset\\testing_data\\images\\82253058_3059.png into spatial data\n",
      "Processed D:\\storage\\training\\dataset\\testing_data\\images\\82253245_3247.png into spatial data\n",
      "Processed D:\\storage\\training\\dataset\\testing_data\\images\\82253362_3364.png into spatial data\n",
      "Processed D:\\storage\\training\\dataset\\testing_data\\images\\82254765.png into spatial data\n",
      "Processed D:\\storage\\training\\dataset\\testing_data\\images\\82491256.png into spatial data\n",
      "Processed D:\\storage\\training\\dataset\\testing_data\\images\\82504862.png into spatial data\n",
      "Processed D:\\storage\\training\\dataset\\testing_data\\images\\82562350.png into spatial data\n",
      "Processed D:\\storage\\training\\dataset\\testing_data\\images\\82573104.png into spatial data\n",
      "Processed D:\\storage\\training\\dataset\\testing_data\\images\\82837252.png into spatial data\n",
      "Processed D:\\storage\\training\\dataset\\testing_data\\images\\83443897.png into spatial data\n",
      "Processed D:\\storage\\training\\dataset\\testing_data\\images\\83553333_3334.png into spatial data\n",
      "Processed D:\\storage\\training\\dataset\\testing_data\\images\\83573282.png into spatial data\n",
      "Processed D:\\storage\\training\\dataset\\testing_data\\images\\83594639.png into spatial data\n",
      "Processed D:\\storage\\training\\dataset\\testing_data\\images\\83624198.png into spatial data\n",
      "Processed D:\\storage\\training\\dataset\\testing_data\\images\\83635935.png into spatial data\n",
      "Processed D:\\storage\\training\\dataset\\testing_data\\images\\83641919_1921.png into spatial data\n",
      "Processed D:\\storage\\training\\dataset\\testing_data\\images\\83772145.png into spatial data\n",
      "Processed D:\\storage\\training\\dataset\\testing_data\\images\\83823750.png into spatial data\n",
      "Processed D:\\storage\\training\\dataset\\testing_data\\images\\83996357.png into spatial data\n",
      "Processed D:\\storage\\training\\dataset\\testing_data\\images\\85201976.png into spatial data\n",
      "Processed D:\\storage\\training\\dataset\\testing_data\\images\\85240939.png into spatial data\n",
      "Processed D:\\storage\\training\\dataset\\testing_data\\images\\85540866.png into spatial data\n",
      "Processed D:\\storage\\training\\dataset\\testing_data\\images\\85629964.png into spatial data\n",
      "Processed D:\\storage\\training\\dataset\\testing_data\\images\\86075409_5410.png into spatial data\n",
      "Processed D:\\storage\\training\\dataset\\testing_data\\images\\86079776_9777.png into spatial data\n",
      "Processed D:\\storage\\training\\dataset\\testing_data\\images\\86220490.png into spatial data\n",
      "Processed D:\\storage\\training\\dataset\\testing_data\\images\\86230203_0206.png into spatial data\n",
      "Processed D:\\storage\\training\\dataset\\testing_data\\images\\86236474_6476.png into spatial data\n",
      "Processed D:\\storage\\training\\dataset\\testing_data\\images\\86244113.png into spatial data\n",
      "Processed D:\\storage\\training\\dataset\\testing_data\\images\\86263525.png into spatial data\n",
      "Processed D:\\storage\\training\\dataset\\testing_data\\images\\86328049_8050.png into spatial data\n",
      "Processed D:\\storage\\training\\dataset\\testing_data\\images\\87086073.png into spatial data\n",
      "Processed D:\\storage\\training\\dataset\\testing_data\\images\\87093315_87093318.png into spatial data\n",
      "Processed D:\\storage\\training\\dataset\\testing_data\\images\\87125460.png into spatial data\n",
      "Processed D:\\storage\\training\\dataset\\testing_data\\images\\87137840.png into spatial data\n",
      "Processed D:\\storage\\training\\dataset\\testing_data\\images\\87147607.png into spatial data\n",
      "Processed D:\\storage\\training\\dataset\\testing_data\\images\\87332450.png into spatial data\n",
      "Processed D:\\storage\\training\\dataset\\testing_data\\images\\87428306.png into spatial data\n",
      "Processed D:\\storage\\training\\dataset\\testing_data\\images\\87528321.png into spatial data\n",
      "Processed D:\\storage\\training\\dataset\\testing_data\\images\\87528380.png into spatial data\n",
      "Processed D:\\storage\\training\\dataset\\testing_data\\images\\87594142_87594144.png into spatial data\n",
      "Processed D:\\storage\\training\\dataset\\testing_data\\images\\89856243.png into spatial data\n",
      "Processed D:\\storage\\training\\dataset\\testing_data\\images\\91814768_91814769.png into spatial data\n",
      "Processed D:\\storage\\training\\dataset\\testing_data\\images\\92380595.png into spatial data\n",
      "Processed D:\\storage\\training\\dataset\\testing_data\\images\\93106788.png into spatial data\n"
     ]
    }
   ],
   "source": [
    "# Execute the load_image command\n",
    "load_image()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis_super_resolution_p3_10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
